#+title: Code TODO
* [ ] Cleanup and optimizations
** [X] Autocompleters needs consts at model level
* [ ] Preprocessors
** [ ] Add user prompt preprocessor (instead of just system prompt) through configuration
** [ ] Add prefix variant of preprocessor to enable caching.
* [ ] Runner
** [ ] Emit event to UI when runner state changes: starts, stops, moves to next node, etc.
** [ ] Add a way to raise error and stop the runner. User should be able to try and continue the runner.
** [ ] Add a way to restrict number of loops when cycling
** [ ] Stopping executor with ctrl+c during input and then starting it with /use does not stop at input, but retries last input node - verify
* [ ] Configuration
** [ ] Update templates and parametrize them via variables
** [ ] Figure out why confirmation override for discovery did not work
** [ ] Tweak architect prompts to not provide solution if there are questions to answer or user explicitly requests one shot solution
** [ ] Tweak prompt for discovery to only work with existing files
Concrete example: files that were read and already in context.
* [ ] Error handling
** [ ] Add exception handlers
*** [ ] Runner
*** [ ] UI
** [ ] Tool calling timeouts and failures
** [ ] Add a way for human to unblock failures by retrying last operation
** [ ] Running another workflow from workflow causes this error to be printed:
Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x131801dc0>
* [X] UI protocol
** [ ] Add syntax name hint to apply formatting
* [ ] Terminal UI
** [X] Always show prompt. Add commands to stop/continue workflows
** [X] When final message is printed, we need to add an extra linebreak. Also, validate streaming line splits with bottom toolbar - it seems to be broken.
** [X] Streaming printing is broken with permanent toolbar visibility
** [ ] When prompt is requested, sometimes toolbar status is not updated
** [ ] Disable know warnings, they mess up UI output
** [ ] Animation does not start after providing feedback message back to Node
** [ ] When LLM is waiting for response, need to show visual feedback to user
** [X] Summarize tool calls for known tools. Create templates for tool calling that map parameters
** [ ] Ability to snoop into tool call details if needed, collapsed by default
** [ ] Better multi-line inputs, don't require esc+enter to send
** [ ] Shortcuts - ESC to stop execution?
** [ ] Show how long individual steps took, in seconds
** [X] Continue after stopping shows prompt second time
** [ ] Continue after stopping does not show agent output upon response, but it's there
** [X] When ctrl+c is pressed while waiting for user input and runner is stopped, need to cancel waiter.
** [ ] Add command to reload config without restart
** [X] Add shortcuts to cancel current input easily
** [ ] Simple styling
*** [X] Color usage
*** [ ] Simple animations
**** [X] Add three spaces to animation to prevent closing ] from animating
**** [X] Ticker is not restarted after stopping for some reason or not always restarted
*** [X] Remove Agent: prefix
** [X] Print node transitions
** [ ] Format tool calls
*** [X] JSON formatter and highlighter
*** [ ] Add a way (tab?) to expand/collapse JSON
*** [ ] Require all tool calls from LLM to have description (Key Objective)
*** [X] Group related tool calls
** [X] Assume markdown as default formatter
** [X] Wrapped lines don't move caret down correctly - next line overwrites it.
** [X] Disable input and drop buffered input between prompts
** [ ] Change workflow execution:
- If workflow is selected, first message sent should start the workflow
- If workflow is stopped after finishing, then sending a new message should resume same workflow from the beginning
- If workflow is canceled, then workflow should start new workflow
** [X] ctrl+c when runner is active does nothing - should stop the runner (verify, might not be the case)
** [?] We might have deadlock somewhere that does not break with ctrl+c
- Added debugging stacktraces for now
** [X] Add file context management once corresponding node is created
** [X] Add file and symbol auto-completes for a last word. Call into Know to do lookup and return most likely candidates. Maybe get complete file and symbol list from Know and create in-memory trigram index for quick lookups.
** [ ] Fix estimated cost calculation
** [ ] Highlighting does not work if ``` opener is not in the beginning of the line
* [ ] Block parsers
** [ ] Diff parsers
*** [ ] Fix stats reporting - only report if file was fully patched in patched section
*** [X] GPT V4A diff format
**** [X] Better error reporting and verify apply patch cycle
**** [X] When multiple chunks match, but we can't match any of the chunks - return all possible lines
**** [X] Add support for multi-blocks where multiple things are getting deleted and added.
**** [X] Add support for multiple patch blocks or provide better instructions
**** [X] Better error instructions when blocks overlap
**** [ ] Allow same file to be mentioned multiple times?
*** [ ] Unified Diff format
* [ ] Nodes
** [ ] Add a node that injects files in context. Add file manager.
*** [X] Needs file auto-complete UI support
*** [X] Show files added and removed, as well as current list of files for /fadd and /fdel
*** [ ] Figure out a way to inject files into patch without apply_patch having access to readfile
*** [ ] [#A] Add a way to manage context and inject files to context while looping in LLM node
** [ ] LLM node
*** [ ] Add tool calling budgets (number of calls, tokens, etc)
*** [ ] Detect tool call loops
*** [ ] Auto-retry on timeout
*** [ ] Auto-retry when throttled
*** [ ] Add stats for the number of tokens in the context and context window limits
*** [ ] Figure out why pricing estimates are all zeroes
*** [ ] Add a way to append text to default system prompt
** [ ] Create RepoMap node - call into Know with provided prompt
** [ ] Create documentation node - read AGENT.md files for all paths that are mentioned in previous messages.
- Have configuration for static message text
- Support one or more explicit paths to be read and inserted into message context
- Append to previous message? Inject into system prompt of LLM node only?
- How do we extract paths reliably?
- Maybe offer a tool?
** [ ] Fan-out node - call other defined tools, collect their results and pass concatenated messages to next tool
** [ ] TODO node - collect plan that is formatted with specific syntax (markdown? function call?)
* [ ] Tools
** [ ] Integrate Know
*** [ ] Add progress report
*** [ ] Figure out how to express 3rd party dependencies and give access
*** [ ] Disable warnings
** [ ] Add pattern matching rules to auto-approve rule calls

** [ ] Shell tool
*** [ ] Need comprehensive tests
*** [ ] Windows shell support
*** [ ] Non-POSIX shell support
*** [ ] PTY support
*** [ ] Containerization
*** [ ] Sandboxing
** [ ] Parallel tool calling support
** [ ] MCP tool support
*** [ ] Pass cwd
** [ ] Add a way to reject tool calling automatically if tools with same parameters were already called
** [ ] Figure out sandboxing
*** [ ] Wrap stdio MCP servers in sandbox
*** [ ] Wrap shell tool in sandbox
** [ ] Apply patch tool - useful for fully-agentic loops
* [ ] Nested workflows support
** [ ] Create API to start a new workflow
- Should start a new runner with new state
- Wait for runner to finish
- Pass all messages through to UI, plumb via parent runner
- UIState should be smart enough to understand it's stacked execution. Need explicit messages to UIState on runner states, such as starting workflow execution, state changes, etc.
** [ ] Create a tool that allows LLMs to call into new workflows
** [ ] Create node that starts a workflow with an input
