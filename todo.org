#+title: Code ToDo

* [ ] UI protocol
** [ ] In-memory bidirectional stream implementation for TUI
** [ ] General requirements
- Send initial configuration: available tools, their node configuration, etc.
- Stream state change messages
- RPC subprotocol for various helpers
  - Request user input and provide response
  - Tab-based autocompletion
  - Send ctrl+c
** [ ] Clearly defined spec
** [ ] Websockets implementation
* [ ] Figure out dependency graph between project, tools
* [ ] Graph
** [X] Rename output to be outcome
** [X] Refactor NodeExecution input_messages and messages. messages should be append only.
** [X] Add a way to override values from a shared config. Options:
- Through special value
- Though path in the settings of <tool_name.node_name.field_name> syntax
- Both?
- Also read from files when file is defined
** [X] Add a way to get node definition from template and override some of the fields from config
** [X] Graph runner
** [X] Add a way to rewind history back to resume from a different point
* [ ] Block parsers
** [ ] Message construction helpers
** [ ] Code parsers
** [ ] Diff parsers
*** [ ] GPT diff format
*** [ ] Fenced diff format
*** [ ] Full file diff format
* [X] Settings
** [X] Settings loader
* [ ] Chat state
** [X] NodeLog, Section, Message
* [ ] Nodes
** [ ] Add project as a parameter to executor
** [ ] [#A] Think how to manage state for a run
- Sometimes we need to maintain the state when additional information is found and added to state
- Sometimes we need to start fresh
- But need to maintain state if looped by human
- Most likely make it a setting of a Node (explicit config). Via enum?
  - Always resets
  - Human resets
  - Never resets
- We need machinery to go back in time and reset graph execution to a specific history
** [X] Base node runner class
** [ ] LLM node
*** [?] Base
**** [ ] Tool configuration
**** [ ] Exposing available tools to LLM from project
*** [ ] Dynamic output selection by LLM
**** [ ] Configurable system prompt extension
**** [ ] Cleanup logic
**** [ ] Re-prompt if answer is not provided
**** [X] Add non-function way of picking next step
**** [ ] Add a way for LLM to request additional user input
*** [ ] Do not add empty message to output
*** [ ] Auto-retry on timeout
*** [ ] Auto-retry when throttled
** [ ] Fan-out node - call other defined tools, collect their results and pass concatenated messages to next tool
** [ ] TODO node - collect plan that is formatted with specific syntax (markdown? function call?)
